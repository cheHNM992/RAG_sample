{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストコーパスをチャンクに分割 - コードに適した分割方法を使用\n",
    "with open('code_gate.py', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# コードに特化したスプリッターを使用（Pythonコードの場合）\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=500,  # コード理解に適したサイズ\n",
    "    chunk_overlap=50  # 適度なオーバーラップで文脈を保持\n",
    ")\n",
    "texts = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d86f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッセージのベクトル化 - コード特化型のエンベディングモデルを使用\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# コードに特化したエンベディングモデルを使用\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "#    model_name='Alibaba-NLP/gte-large-code',  # コード特化型エンベディングモデル　★存在しないモデル\n",
    "    model_name='BAAI/bge-small-en-v1.5',  # 軽量で高性能な汎用エンベディングモデル\n",
    "#    model_name='microsoft/codebert-base',  # コード用に事前学習されたモデル\n",
    "#    model_name='nomic-ai/nomic-embed-text-v1',  # 高性能な汎用エンベディング\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb3dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# データベースの保存\n",
    "db = FAISS.from_texts(texts, embeddings)\n",
    "db.save_local('code.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97725b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 保存したデータベースの読み込み\n",
    "db = FAISS.load_local('code.db', embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieverの設定 - コード検索向けのパラメータ調整\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # コードの多様な部分を取得するためにMMRを使用\n",
    "    search_kwargs={\n",
    "        \"k\": 4,  # 取得するドキュメント数\n",
    "        \"fetch_k\": 15,  # MMR探索用の初期取得数 \n",
    "        \"lambda_mult\": 0.6  # 関連性と多様性のバランス\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの準備\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# DeepSeek Coderモデルを使用\n",
    "model_id = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"  # 利用可能なGPUに自動的に配置\n",
    ").eval()\n",
    "\n",
    "# DeepSeek Coderに適したパイプライン設定\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,  # より詳細な回答を可能に\n",
    "    do_sample=True,\n",
    "    temperature=0.2,  # 低温度で一貫性を向上\n",
    "    top_p=0.92,  # 確率の高い選択肢に集中\n",
    "    repetition_penalty=1.1,  # DeepSeekは繰り返しが少ないのでペナルティ軽減\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek Coderに適したプロンプトテンプレート\n",
    "# DeepSeekモデルの推奨プロンプト形式に調整\n",
    "template = \"\"\"<问题>\n",
    "以下のPythonコードを参照して、質問に答えてください:\n",
    "\n",
    "{context}\n",
    "\n",
    "質問: {question}\n",
    "</问题>\n",
    "\n",
    "<答案>\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template_format=\"f-string\"\n",
    ")\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=HuggingFacePipeline(pipeline=pipe),\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f550f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行例\n",
    "\n",
    "def clean_response(response):\n",
    "    \"\"\"DeepSeekの応答から必要な部分を抽出し、整形する\"\"\"\n",
    "    result = response['result'].strip()\n",
    "    # DeepSeekモデル特有の終了トークンがあれば削除\n",
    "    if \"</答案>\" in result:\n",
    "        result = result.split(\"</答案>\")[0].strip()\n",
    "    return result\n",
    "\n",
    "# テスト実行と評価\n",
    "questions = [\n",
    "    \"どんなプログラミング言語で書かれていますか？\",\n",
    "    \"定義されている関数を全てリストアップして下さい\",\n",
    "    \"AND関数は何回呼ばれますか？\",\n",
    "    \"このプログラムがやっていることは何ですか？\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"質問: {q}\")\n",
    "    ans = qa.invoke(q)\n",
    "    cleaned_ans = clean_response(ans)\n",
    "    print(f\"回答: {cleaned_ans}\")\n",
    "    print(\"\\n参照されたドキュメント:\")\n",
    "    for i, doc in enumerate(ans['source_documents'][:2]):  # 最初の2つだけ表示\n",
    "        print(f\"ドキュメント {i+1}: {doc.page_content[:100]}...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 精度評価関数\n",
    "def evaluate_answers(questions, expected_answers, model_responses):\n",
    "    \"\"\"回答の精度を簡易評価する関数\"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    # 回答をベクトル化して類似度を計算\n",
    "    def get_similarity(ans1, ans2):\n",
    "        # 簡易的なベクトル化と類似度計算\n",
    "        # 実際のシステムではより高度な方法を使用すべき\n",
    "        return cosine_similarity([ans1], [ans2])[0][0]\n",
    "    \n",
    "    results = []\n",
    "    for q, exp, res in zip(questions, expected_answers, model_responses):\n",
    "        sim = get_similarity(exp, res)\n",
    "        results.append({\n",
    "            \"question\": q,\n",
    "            \"similarity\": sim,\n",
    "            \"pass\": sim > 0.7  # 70%以上の類似度を合格とする\n",
    "        })\n",
    "    \n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
